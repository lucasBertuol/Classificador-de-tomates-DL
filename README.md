# Classificador-de-tomates-DL
Este projeto foi desenvolvido como estudo de caso did√°tico para avaliar e comparar o desempenho de abordagens de **transfer learning** com redes neurais de Deep Learning treinadas do zero na tarefa de reconhecimento de **matura√ß√£o de tomates** em imagens. Essa √© uma atividade que faz parte do Bootcamp  BairesDev - Machine Learning Training oferecido pela [DIO](https://www.dio.me/) e foi realizada a partir de um tutorial para transfer learning disponibilizado pelo meu professor acess√≠vel nesse [link](https://colab.research.google.com/github/kylemath/ml4a-guides/blob/master/notebooks/transfer-learning.ipynb#scrollTo=VWWN-FPLYoZs) .

##  Objetivos üéØ
 - Demonstrar, com exemplos pr√°ticos, como o transfer learning pode superar ou igualar modelos criados do zero, especialmente em cen√°rios com poucos dados. A tarefa central √© classificar imagens em duas categorias: **tomates maduros** e **tomates verdes**.
 - Comparar diferentes abordagens de **Transfer learning** e **Feature extraction**
 
 ## Estrutura do projeto üóÇÔ∏è
 -   **Tomato_Classifier_1.ipynb:**  Transfer learning com apenas a camada de sa√≠da substitu√≠da; (~8.000 par√¢metros trein√°veis)
    
-   **Tomato_Classifier_2.ipynb:**  Transfer learning com head customizado;  (~3 milh√µes de par√¢metros trein√°veis)
    
-   **Ambos notebooks incluem tamb√©m a arquitetura e treinamento de um modelo do zero para compara√ß√£o**
    
-   **dataset/train/**  e  **dataset/test/**: Imagens organizadas por classe: ripe (maduro) e unripe (verde)
    
-   **Imagens dos resultados:**  Gr√°ficos de loss e acur√°cia ao longo de 20 epochs de treinamento.
## Recursos Utilizados ‚öíÔ∏è

-   **Linguagem:**  Python 3.x 
    
-   **Framework:**  TensorFlow (Keras API)
    
-   **Execu√ß√£o:**  Google Colab (Possui bibliotecas necess√°rias j√° instaladas e interface amig√°vel. Executei em runtime local para maior velocidade)
    
-   **Modelo base para Transfer Learning:**  rede VGG16 pr√©-treinada em ImageNet (diretamente via keras.applications)
- **Dataset com imagens de tomates**
## Dataset üìä
Eu utilizei um dataset do Kaggle j√° pronto contendo imagens de tomates em v√°rios est√°gios de matura√ß√£o, dispon√≠vel nesse [link](https://www.kaggle.com/datasets/sumn2u/riped-and-unriped-tomato-dataset). √â fundamental garantir que os dados estejam rotulados e organizados corretamente, e que haja um equil√≠brio razo√°vel entre as classes para evitar vieses no aprendizado. Por isso, os dados est√£o organizados dessa forma:

    Dataset/
	    Test/ # 110 no total
		    Ripe/ # 55 imagens
		    Unripe/ # 55 imagens
		    
	    Train/ # 329 no total
		    Ripe/ # 155 imagens
		    Unripe/ # 174 imagens
Algumas centenas de imagens j√° s√£o suficientes para treinar a nossa rede, por se tratar de uma tarefa de classifica√ß√£o simples, mas o ideal seria possuir mais imagens. Por isso, vamos aplicar **data augmentation** para melhorar a qualidade do nosso dataset. 
## Metodologia üìñ
### Data augmentation 
Eu apliquei esse m√©todo para artificialmente aumentar o **tamanho** e a **variedade** do conjunto de dados de treino (ds_treino). O objetivo √© melhorar a robustez e previnir o overfitting (quando o modelo memoriza os dados de treino em vez de aprender padr√µes gerais), aumentando a capacidade de **generaliza√ß√£o** dos modelos. Foram aleatoriamente alterados a orienta√ß√£o, rota√ß√£o, zoom e contraste das imagens. 

### Transfer learning
Transfer learning √© uma t√©cnica de Machine Learning em que aproveitamos o **aprendizado** de um modelo de IA para treinar outro modelo. Ao construir um modelo do zero, √© necess√°rio possuir um dataset com uma grande quantidade e variabilidade de dados para obter bons resultados. Por outro lado, ao utilizar um modelo pr√©-treinado podemos usar o conhecimento adquirido em determinada tarefa e aplicar a um novo problema relacionado. Dessa forma, economizam-se recursos computacionais e se torna poss√≠vel alcan√ßar uma boa **precis√£o** mesmo com um dataset limitado, o que √© muito √∫til para o nosso caso.  
Dentro do Transfer Learning, existem duas estrat√©gias principais:  **Feature extraction**  e  **Fine-tuning**. Essas estrat√©gias s√£o consideradas subdivis√µes ou abordagens distintas dentro do Transfer Learning.

-   **Feature extraction**: utiliza-se o modelo pr√©-treinado como um extrator de features, aproveitando as representa√ß√µes aprendidas nas primeiras camadas e, geralmente, apenas treinando novas camadas "head" (de classifica√ß√£o) para a tarefa desejada. Mais eficaz para datasets pequenos. 
    
-   **Fine-tuning**: al√©m de adicionar ou treinar o head, parte ou todas as camadas do modelo pr√©-treinado passam por um ajuste fino (ou seja, s√£o retreinadas), permitindo ao modelo adaptar-se melhor √† nova tarefa ou dom√≠nio. √â necess√°rio possuir dados suficientes para n√£o causar overfitting. 

Devido ao dataset pequeno, eu optei por realizar Feature extraction. Entretanto, eu n√£o tinha certeza se as features de alto n√≠vel do ImageNet seriam relevantes para um caso espec√≠fico como o de matura√ß√£o de tomates, apesar de se tratarem de objetos do mundo real. Por essa raz√£o, foram exploradas duas formas distintas de realizar a Feature Extraction:
### M√©todo 1: substituir a camada de sa√≠da da rede pr√©-treinada
Esse m√©todo foi usado no arquivo **Tomato_Classifier_1**. Nele eu treino novamente a camada de decis√£o final da VGG16, adaptando-a para as minhas classes. Apenas essa √∫ltima camada da rede permance trein√°vel, o resto da rede fica cogelada.  A vantagem dessa estrat√©gia √© aproveitar as features gen√©ricas do ImageNet, treinar mais rapidamente e reduzir o risco de overfitting com o nosso dataset pequeno. Em contrapartida, a adapta√ß√£o do modelo fica limitada se as features pr√©-existentes n√£o forem suficientemente representativas para a minha tarefa.

**Processo:**
1. Eu importei a rede VGG16 com pesos pr√©-treinados em ImageNet(`weights='imagenet'`), e com o head original `include_top=True`
2. A seguir, eu criei uma nova camada densa para o meu n√∫mero de classes (2), usando softmax como ativa√ß√£o. 
3. Ent√£o, troquei a camada de classifica√ß√£o final da VGG16, uma camada softmax de 1000 neur√¥nios correspondente √† ImageNet, por essa nova camada de 2 neur√¥nios. Com isso,  crei uma nova rede chamada `model_new` 
4. O pr√≥ximo ajuste foi congelar todas as camadas da rede `model_new`, exceto a √∫ltima. Para compilar o modelo, usei a fun√ß√£o de perda `categorical_crossentropy` (adequada para medir a performance em tarefas de classifica√ß√£o), otimizador `adam` e m√©tricas para `accuracy`

imagem do model.summary()
### M√©todo 2: treinar um novo classificador para o modelo
Esse m√©todo foi usado no arquivo **Tomato_Classifier_2**. As features de alto n√≠vel do modelo pr√©-treinado podem n√£o ser relevantes para classificar tomates. √â por isso que dessa vez eu utilizo o meu pr√≥prio head customizado, mais complexo e com mais par√¢metros para se adaptar √† minha tarefa espec√≠fica. Simultaneamente, as features mais gen√©ricas de camadas anteriores da VGG16 servem como base para o novo classificador. Essa rede pode aprender combina√ß√µes mais ricas e n√£o lineares das features extra√≠das, o que pode levar a um melhor desempenho, mas tamb√©m aumenta o risco de overfitting se os dados forem insuficientes. Todos os blocos convolucionais do modelo s√£o congelados, apenas o novo head √© trein√°vel. 

**Processo**:
1. Importei a VGG16 novamente com pesos pr√©-treinados mas dessa vez sem o head original `include_top=False`
2. Importei o resto da rede VGG16 com pesos congelados `vgg(inputs, training=False)`
3. Criei um head customizado para a minha tarefa de classifica√ß√£o (camada Flatten + Dense + Dropout) e uma nova camada de sa√≠da (com 2 neur√¥nios). Nomeei a nova rede como `model_new`
4. Compilei o modelo da mesma forma, usando `loss='categorical_crossentropy'`, `optmizer='adam'` e `metrics='accuracy'`
Detalhes do modelo:

imagem do model.summary()

Em todos os modelos eu usei um batch_size de 128 e treinei por 20 epochs. 
### Modelo de controle: rede neural treinada do zero
Com a inten√ß√£o de comparar a efici√™ncia e os resultados de se utilizar Transfer learning em rela√ß√£o com um modelo de IA sem nenhum conhecimento pr√©vio, eu desenvolvi um modelo chamado `model_scratch`. Detalhes do modelo:

imagem do model.summary()
## Resultados üìà
A seguir, est√£o os gr√°ficos de **perda** e **precis√£o** obtidos com cada modelo. Em azul, o modelo treinado do zero e em laranja os respectivos modelos com Transfer Learning. 

**Tomato_classifier_1**

imagem Tomato_Classifier_1_output 

84.55% de precis√£o com 0.44 de perda no modelo de Transfer Learning contra 60.91% de precis√£o com 1.20 de perda no modelo treinado do zero. 

**Tomato_classifier_2**

imagem Tomato_Classifier_2_output 

81.82% de precis√£o com 0.76 de perda no modelo de Transfer Learning contra 66.36% de precis√£o com 0.75 de perda no modelo treinado do zero. 

Observando os gr√°ficos √© poss√≠vel concluir que o modelo treinado do zero (`model_scratch`) n√£o foi capaz de realizar um treinamento robusto mesmo com seus 1.2 milh√µes de neur√¥nios e manteve uma precis√£o relativamente constante ao longo das 20 epochs. Ao aplicar o Transfer Learning, conseguimos observar uma melhora instant√¢nea nos resultados do modelo, em decorrer do conhecimento pr√©vio da rede pr√©-treinada sendo utilizada.
Por outra perspectiva, analisamos que apesar do modelo usado em Tomato_Classifier_1 ser **menos complexo**, com apenas 8.194 neur√¥nios, ele foi capaz de atingir uma **melhor performance** entre todos os modelos. Em contrapartida, o modelo mais sofisticado empregado em Tomato_Classifier_2 apesar de possuir um n√∫mero muito maior de neur√¥nios trein√°veis n√£o foi capaz de alcan√ßar uma melhor acur√°cia. Ao se tornar mais complexa, essa rede se tornou mais suscet√≠vel ao overfitting. 

Classifier_2_overfitting
A precis√£o nos dados de treino aumentou enquanto nos dados de teste a precis√£o diminuiu nas √∫ltimas epochs.

Isso nos leva √† conclus√£o de que nem sempre um modelo com mais par√¢metros ser√° o mais eficaz, pois em geral quanto maior a quantidade de par√¢metros maior √© a quantidade de dados necess√°ria para alimentar o modelo. 
## Conclus√£o üçÖ
Por fim, conclui-se que mesmo com um dataset pequeno √© poss√≠vel melhorar a acur√°cia de uma rede de Deep learning ao utilizar **Data augmentation** e **Transfer Learning**. Com apenas **0,68%** dos neur√¥nios usados na rede feita do zero, obtemos uma maior precis√£o. Isso nos permite economizar tempo e recursos computacionais. Comparando as redes 1 e 2 constatamos que as features extra√≠das pela rede VGG16 s√£o suficientemente **relevantes**, tornando o classificador mais simples do Tomato_Classifier_1 a abordagem mais **eficaz** para este problema espec√≠fico. 
Para escolher a melhor estrat√©gia de treinamento, √© essencial considerar o **tamanho** do seu dataset e o **n√≠vel de similaridade** com a rede pr√©-treinada de sua escolha, pois √© isso que ir√° definir o melhor caminho para conquistar bons resultados com Transfer learning.  
## Contato üìß
Estou aberto a contribui√ß√µes e sugest√µes de melhoria!

Email: lucascondessabertuol@gmail.com

LinkedIn: https://www.linkedin.com/in/lucasbertuol/
